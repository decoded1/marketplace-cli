# Marketplace CLI - Continuation Plan

**Date**: November 5, 2025 (Updated)
**Location**: Pine Hill, NJ 08021
**Status**: Facebook lane resolved, designing AI-collaborative architecture

---

## ðŸŽ¯ PROJECT VISION: AI-COLLABORATIVE MARKETPLACE AGGREGATOR

### Core Concept
Build an all-encompassing marketplace CLI tool designed for **AI-to-human collaboration**, where:
- **User provides natural language queries** â†’ **AI translates to CLI parameters**
- **AI pulls levers (CLI flags/options)** â†’ **CLI code captures data and creates CSV/JSON files**
- **Standardized data format** across all platforms (OfferUp, Craigslist, Facebook)
- **Two-tier scraping strategy**: Fast search overview â†’ Detailed on-demand scraping
- **Human reviews filtered results** manually or via LLM analysis
- **URLs tracked** for all captures to enable follow-up

**Key Distinction**: AI is the **controller** (interprets natural language, sets parameters, executes commands), not the **processor** (the CLI tool does data capture, parsing, and file generation)

### User's Vision Quote
*"I want to build this all-encompassing tool to make it easy for AI's like yourself specifically to be able to use it in collaborative way where I give you the details on what I need to search for and where I need to search for it specifically what town whatever with the item I'm looking for is and then you are to go on the hunt or capture large swaths of data and neatly organized them into a CSV file or a Json file that way, I can review the listings either manually or feed them into a suitable model that is able to read large amounts of data and burn lots of tokens, but still be able to pick out listings that would be suitable."*

---

## Current Situation

**Lane Status** (as of November 5, 2025):
- âœ… **Facebook Marketplace**: RESOLVED (37 results, session persistence working)
- âœ… **OfferUp**: RESOLVED (43 results via pyOfferUp)
- âœ… **Craigslist**: RESOLVED (CraigslistScraper with tier 1 + tier 2, filtering, drive-distance support)
- âŒ **PyCraigslist**: ELIMINATED (Docker requirement not needed - CraigslistScraper is sufficient)

### Cross-Market Feature Alignment

| Capability | Craigslist | Facebook Marketplace | OfferUp | Notes |
|------------|------------|----------------------|---------|-------|
| Tierâ€¯1 search (titles/prices/URLs) | âœ… | âœ… | âœ… | Facebook via Playwright; OfferUp via pyOfferUp client |
| Tierâ€¯2 detail capture (description, images, seller info) | âœ… | âš ï¸ (first listing proven; automation pending) | â¬œ | Facebook detail scrape works but still manual for multi-item pass |
| Drive-distance & duration (OpenRouteService) | âœ… | â¬œ | â¬œ | Craigslist detail pages expose map coords; other lanes not yet wired |
| Straight-line distance fallback | âœ… | â¬œ | â¬œ | Craigslist tierâ€¯1 geodesic estimate implemented |
| Location radius / ZIP targeting | âœ… (postal + `search_distance`) | âœ… (`radius`, lat/lon) | âœ… (`pickup_distance`) | All three platforms support radius once adapter wiring is complete |
| Price filtering | âœ… | âš ï¸ (UI supports filters; automation TBD) | âš ï¸ (pyOfferUp supports max price; needs CLI hook) | |
| Noise filtering (buyer/repair/accessory suppression) | âœ… | â¬œ | â¬œ | Craigslist tierâ€¯1 filters applied; others still raw |
| Session / login management | N/A (public access) | âœ… (storage state) | âš ï¸ (API key/credentials optional) | OfferUp relies on API creds if user wants private data |
| Category coverage mapping | âœ… (for-sale category table) | âš ï¸ (core marketplace categories logged) | âš ï¸ (pyOfferUp exposes categories) | |

Legend: âœ… implemented, âš ï¸ partial / manual, â¬œ not started, N/A not required.

---

## ðŸ—ï¸ AI-COLLABORATIVE ARCHITECTURE REQUIREMENTS

### Natural Language Interface Design
**Goal**: AI should translate user's natural language into executable search parameters

**Example User Input**:
> "Find me nintendo switches under $200 in Philadelphia"

**AI Translation**:
```python
{
  "query": "nintendo switch",
  "location": "Philadelphia, PA",
  "max_price": 200,
  "platforms": ["facebook", "offerup", "craigslist"],
  "capture_mode": "tier1"  # or "tier2" for detailed
}
```

**AI Capabilities Required**:
- Location normalization (Philadelphia â†’ philly/Newark/jerseyshore codes)
- Price extraction from natural language ($200, under 200, etc.)
- Query term extraction and cleaning
- Platform selection (individual or "all")
- Mode selection (tier1 fast scan vs tier2 detailed)

---

### Two-Tier Scraping Strategy

#### **Tier 1: Fast Search Overview** (Primary Scraping)
**Purpose**: "Get a lay of the land" - quick scan of available listings

**Data Captured**:
- Title
- Price
- Location
- Thumbnail image URL
- Listing URL (CRITICAL for tier 2)
- Platform source (facebook/offerup/craigslist)
- Timestamp of capture

**Performance**:
- Fast: 1 page load per search
- Low detection risk
- Suitable for bulk scanning

**Use Case**: Initial discovery phase, price comparison, availability checking

#### **Tier 2: Detailed Scraping** (Secondary, On-Demand)
**Purpose**: "Get more insight on a second pass" - deep dive into specific listings

**Additional Data Captured**:
- Full description (multi-paragraph)
- Item condition (New, Used - Like New, etc.)
- Brand information
- Multiple images (full gallery)
- Seller name
- Seller reputation/join date
- Exact location with map
- Related/similar items

**Performance**:
- Slower: N+1 page loads (clicks through each listing)
- Higher detection risk
- Suitable for shortlisted items

**Use Case**: Final evaluation before purchase, comparing specific items

---

### Standardized Data Format (Cross-Platform)

**JSON Structure** (for AI processing):
```json
{
  "capture_session": {
    "id": "uuid-here",
    "timestamp": "2025-11-05T14:30:00Z",
    "query": "nintendo switch",
    "location": "Philadelphia, PA",
    "max_price": 200,
    "platforms": ["facebook", "offerup", "craigslist"],
    "tier": 1,
    "total_results": 123
  },
  "listings": [
    {
      "id": "unique-id",
      "platform": "facebook",
      "title": "Nintendo Switch OLED - Like New",
      "price": 250.00,
      "currency": "USD",
      "location": "Philadelphia, PA",
      "distance_miles": 5.2,
      "url": "https://facebook.com/marketplace/item/12345",
      "image_url": "https://...",
      "posted_date": "2025-11-01",
      "tier1_data": {
        "snippet": "Brief preview text..."
      },
      "tier2_data": null  // Populated only if detailed scrape run
    }
  ]
}
```

**CSV Format** (for human review):
```csv
platform,title,price,location,url,posted_date,condition,image_url
facebook,"Nintendo Switch OLED",250,"Philadelphia, PA","https://...",2025-11-01,like_new,"https://..."
offerup,"Switch Bundle",200,"Newark, NJ","https://...",2025-11-02,used,"https://..."
```

**Field Mapping Across Platforms**:
| Standard Field | Facebook | OfferUp | Craigslist |
|----------------|----------|---------|------------|
| `title` | Listing title | Item name | Post title |
| `price` | Price span | Price field | Price tag |
| `location` | Seller location | Distance-based | Area name |
| `url` | `/marketplace/item/{id}` | `/item/detail/{id}` | Post URL |
| `condition` | Condition field | Condition tag | In description |
| `posted_date` | Listed date | Posted time | Post date |

---

### CLI Command Structure for AI

**Design Philosophy**: Commands should be intuitive for AI to construct from natural language

**Basic Search** (Tier 1):
```bash
marketplace-cli search \
  --query "nintendo switch" \
  --location "Philadelphia, PA" \
  --max-price 200 \
  --platforms facebook offerup craigslist \
  --output results.json \
  --format json
```

**Detailed Scrape** (Tier 2):
```bash
marketplace-cli details \
  --input results.json \
  --indices 0 5 12 \
  --output detailed.json
```

**Multi-Platform Aggregation**:
```bash
marketplace-cli search \
  --query "nintendo switch" \
  --location "Philadelphia, PA" \
  --max-price 200 \
  --all-platforms \
  --output aggregated.csv \
  --format csv
```

**Session-Based Workflow** (for large captures):
```bash
# Start capture session
marketplace-cli session start "nintendo_switches_nov_2025"

# Add searches to session
marketplace-cli session add facebook "nintendo switch" --max-price 200
marketplace-cli session add offerup "nintendo switch" --max-price 200
marketplace-cli session add craigslist "nintendo switch" --max-price 200

# Execute all searches
marketplace-cli session run

# Get detailed info for specific items
marketplace-cli session details 1 5 12 --tier 2

# Export session results
marketplace-cli session export --format csv --output results.csv
```

---

### URL Tracking and Capture Management

**Session Metadata Storage**:
```json
{
  "sessions": [
    {
      "id": "nintendo_switches_nov_2025",
      "created": "2025-11-05T14:30:00Z",
      "query_params": {...},
      "results_file": "data/sessions/nintendo_switches_nov_2025.json",
      "tier1_count": 123,
      "tier2_count": 15,
      "platforms": ["facebook", "offerup", "craigslist"]
    }
  ]
}
```

**URL Tracking Benefits**:
1. **Reproducibility**: Re-visit exact listings later
2. **Tier 2 Activation**: Click through URLs for detailed scraping
3. **Price Monitoring**: Check same URLs over time for price drops
4. **Deduplication**: Same item across platforms (URL comparison)
5. **LLM Context**: Provide URLs to models for visual analysis

---

## USER'S PRIMARY OBJECTIVES (UPDATED)

### âœ… Priority 1: Facebook Marketplace - RESOLVED
**Status**: âœ… COMPLETE

**Achievements**:
1. âœ… Session persistence working (`facebook_session.json`)
2. âœ… Manual login supported (90-second window)
3. âœ… Marketplace-specific search box identified
4. âœ… 37 listings scraped successfully
5. âœ… Tier 1 data extraction validated (title, price, location, URL)
6. âœ… Tier 2 approach validated (click-through works for first listing)

**Implementation**: [test_facebook_automated_search.py](test_scripts/test_facebook_automated_search.py)

**Key Learnings**:
- Use base Marketplace URL + search box (not direct search URLs)
- Session persistence avoids re-login
- Page interaction > direct URL navigation (crashes less)
- Chromium 140+ required for macOS 26.0.1 compatibility

---

### âœ… Priority 2: Craigslist Access - RESOLVED
**Status**: âœ… COMPLETE

**Recent Wins**:
- Tierâ€¯1 + Tierâ€¯2 flows validated for South Jersey/Philadelphia queries
- Drive-time helper integrated (OpenRouteService with key rotation, geodesic fallback)
- Tierâ€¯1 straight-line estimates + Tierâ€¯2 routed distances now recorded on every listing
- IP-based safety check prompts when working away from the saved origin
- Aggressive listing filters drop buyer/reseller, repair, and accessory noise automatically

**Implementation**: [test_craigslist_tier1.py](test_scripts/test_craigslist_tier1.py) + [CraigslistScraper patched](craigslist_scraper_patched/)

**Future Enhancements** (post-CLI):
- Mirror the coordinate capture + drive metrics in Facebook and OfferUp detail scrapers

**Action Items**:
- [x] Test CraigslistScraper with multiple search terms and categories (tierâ€¯1 / tierâ€¯2)
- [x] Validate tier 1 data extraction (title, price, location, URL) with spam filtering
- [x] Test tier 2 approach (click-through, map coordinates, drive metrics)
- [x] Map Craigslist tier 1 + tier 2 fields to the standardized schema (distance fields included)
- [x] Compare results and counts against manual Craigslist browsing (spot checks performed during testing)
- [x] Document Craigslist quirks (free section, category codes, buyer/accessory filter patterns)
- [ ] Extend coordinate capture + drive metrics to Facebook and OfferUp (tierâ€¯2) once Craigslist lane is signed off

**Craigslist "For Sale" Category Reference (South Jersey Site)**  
Purpose: give the controller a ready map of category abbreviations and endpoints so it can pivot searches dynamically without re-scraping the homepage. Each entry maps to `https://southjersey.craigslist.org/search/{category}` and accepts the same filters already supported (`postal`, `search_distance`, `min_price`, `max_price`, `condition`, etc.).

`for sale` root: `/search/sss`

- `antiques` `/search/ata`
- `appliances` `/search/ppa`
- `arts+crafts` `/search/ara`
- `atv/utv/sno` `/search/sna`
- `auto parts` `/search/pta`
- `aviation` `/search/ava`
- `baby+kid` `/search/baa`
- `barter` `/search/bar`
- `beauty+hlth` `/search/haa`
- `bike parts` `/search/bip`
- `bikes` `/search/bia`
- `boat parts` `/search/bpa`
- `boats` `/search/boo`
- `books` `/search/bka`
- `business` `/search/bfa`
- `cars+trucks` `/search/cta`
- `cds/dvd/vhs` `/search/ema`
- `cell phones` `/search/moa`
- `clothes+acc` `/search/cla`
- `collectibles` `/search/cba`
- `computer parts` `/search/syp`
- `computers` `/search/sya`
- `electronics` `/search/ela`
- `farm+garden` `/search/gra`
- `free` `/search/zip`
- `furniture` `/search/fua`
- `garage sale` `/search/gms`
- `general` `/search/foa`
- `heavy equip` `/search/hva`
- `household` `/search/hsa`
- `jewelry` `/search/jwa`
- `materials` `/search/maa`
- `motorcycle parts` `/search/mpa`
- `motorcycles` `/search/mca`
- `music instr` `/search/msa`
- `photo+video` `/search/pha`
- `rvs+camp` `/search/rva`
- `sporting` `/search/sga`
- `tickets` `/search/tia`
- `tools` `/search/tla`
- `toys+games` `/search/taa`
- `trailers` `/search/tra`
- `video gaming` `/search/vga`
- `wanted` `/search/waa`
- `wheels+tires` `/search/wta`

Usage pattern: build URLs through the patched `build_url()` helper using `city='southjersey'`, the category code above, and any additional filters. This lets the AI decide whether to run broad sweeps (`sss`) or target sub-verticals (e.g., `zip` for free, `ela` for electronics) without extra discovery steps.

**Free section nuance**: Navigating to `/search/zip` loads the dedicated â€œfree stuffâ€ category. The additional links (e.g., `/search/fua?free=1`) are cross-category filters showing items flagged as free within another vertical (furniture, farm+garden, etc.). Toggle these by passing `extra_params={'free': 1}` together with the target category code when calling `build_url(...)`.

**Decision**: CraigslistScraper is production-ready with all required features. PyCraigslist eliminated due to Docker complexity without sufficient benefit.

---

### ðŸŽ¯ Priority 3: Build Unified CLI Architecture
**Status**: âœ… ALL LANES RESOLVED - READY TO BUILD CLI

**User Requirement**: *"I do not want to create the unified CLI until we can iron out some of these kinks"*

**ALL LANES NOW RESOLVED**:
- âœ… **Facebook Lane**: COMPLETE (tier 1 + tier 2 working with session persistence)
- âœ… **OfferUp Lane**: COMPLETE (pyOfferUp working perfectly)
- âœ… **Craigslist Lane**: COMPLETE (CraigslistScraper with tier 1 + tier 2, filtering, drive-distance)

**Design Principles for AI Collaboration**:
1. **Natural Language Parsing**: AI extracts query, location, price from user text
2. **Toggles/Levers**: Standardized flags AI can manipulate
3. **Flexible Platform Selection**: Search one or all platforms
4. **Session Management**: Track large data captures with IDs
5. **Dual Output**: JSON (for AI analysis) + CSV (for human review)
6. **URL Preservation**: All listings retain URLs for follow-up

---

## ~~Docker Questions~~ - RESOLVED: Docker Not Needed

**Decision**: PyCraigslist eliminated entirely. CraigslistScraper provides all required functionality without Docker complexity.

**Rationale**:
- CraigslistScraper works reliably for tier 1 and tier 2 scraping
- Drive-distance integration already implemented
- Noise filtering and category support complete
- No Docker dependencies = simpler deployment and maintenance
- All three lanes now Docker-free

---

## ðŸ› ï¸ IMPLEMENTATION ROADMAP

### Phase 1: Validate All Three Marketplace Lanes âœ… COMPLETE
**Goal**: Ensure Facebook, OfferUp, and Craigslist all work reliably before building unified CLI

**Completed Tasks**:
- [x] Facebook Marketplace with session persistence and manual login (tier 1 + tier 2)
- [x] OfferUp via pyOfferUp library (tier 1)
- [x] Craigslist tier 1 search with spam filtering and distance estimates
- [x] Craigslist tier 2 detail scraping with drive-time calculations
- [x] OpenRouteService integration with geodesic fallback
- [x] Comprehensive category mapping for all platforms
- [x] Standardized data schema design across platforms
- [x] URL tracking for tier 2 follow-up
- [x] PyCraigslist evaluation and elimination decision

**All Success Criteria Met**:
- âœ… All three platforms return reliable results
- âœ… Tier 1 and tier 2 workflows validated
- âœ… Data extraction working for all key fields
- âœ… No Docker dependencies required
- âœ… Ready to build unified CLI architecture

---

### Phase 2: Unified CLI Architecture Design ðŸš€ NEXT PHASE
**Goal**: Build AI-collaborative CLI with natural language interface

**Status**: Ready to begin - all marketplace lanes validated and working

**Core Modules**:

#### **Module 1: Natural Language Parser** (`nlp_parser.py`)
**Purpose**: Translate user's natural language to search parameters

```python
from dataclasses import dataclass
from typing import Optional, List

@dataclass
class SearchQuery:
    query: str
    location: str
    max_price: Optional[float]
    min_price: Optional[float]
    platforms: List[str]  # ["facebook", "offerup", "craigslist"] or ["all"]
    tier: int  # 1 or 2
    radius_miles: Optional[int]
    category: Optional[str]

def parse_natural_language(user_input: str) -> SearchQuery:
    """
    Parse natural language input into structured search parameters.

    Examples:
    - "nintendo switch under $200 in Philadelphia"
      â†’ SearchQuery(query="nintendo switch", location="Philadelphia, PA",
                    max_price=200, platforms=["all"], tier=1)

    - "find iphones on facebook marketplace near me under 300"
      â†’ SearchQuery(query="iphone", location="Philadelphia, PA",
                    max_price=300, platforms=["facebook"], tier=1)

    - "get details for listing #5 from last search"
      â†’ Triggers tier 2 scrape for specific listing
    """
    pass
```

#### **Module 2: Platform Adapters** (`adapters/`)
**Purpose**: Standardize interface across all platforms

```python
# adapters/base.py
from abc import ABC, abstractmethod
from typing import List, Dict

class MarketplaceAdapter(ABC):
    @abstractmethod
    def search_tier1(self, query: str, location: str, max_price: float) -> List[Dict]:
        """Fast search - get overview of listings"""
        pass

    @abstractmethod
    def search_tier2(self, listing_urls: List[str]) -> List[Dict]:
        """Detailed search - click through to individual listings"""
        pass

    @abstractmethod
    def normalize_data(self, raw_data: Dict) -> Dict:
        """Convert platform-specific data to standardized format"""
        pass

# adapters/facebook.py
class FacebookAdapter(MarketplaceAdapter):
    def search_tier1(self, query, location, max_price):
        # Use test_facebook_automated_search.py logic
        pass

    def search_tier2(self, listing_urls):
        # Use test_facebook_listing_details.py logic
        pass

# adapters/offerup.py
class OfferUpAdapter(MarketplaceAdapter):
    def search_tier1(self, query, location, max_price):
        # Use pyOfferUp library
        pass

# adapters/craigslist.py
class CraigslistAdapter(MarketplaceAdapter):
    def search_tier1(self, query, location, max_price):
        # Use CraigslistScraper
        pass
```

#### **Module 3: Session Manager** (`session_manager.py`)
**Purpose**: Track capture sessions, manage data persistence

```python
import json
import uuid
from datetime import datetime
from pathlib import Path

class CaptureSession:
    def __init__(self, session_id: str = None):
        self.session_id = session_id or str(uuid.uuid4())
        self.created_at = datetime.now()
        self.searches = []
        self.results = []
        self.data_dir = Path("data/sessions") / self.session_id
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def add_search(self, platform: str, query: SearchQuery):
        """Add search to session queue"""
        self.searches.append({
            "platform": platform,
            "query": query,
            "timestamp": datetime.now().isoformat()
        })

    def save_results(self, results: List[Dict], tier: int):
        """Save results to session directory"""
        output_file = self.data_dir / f"tier{tier}_results.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)

    def export_csv(self, output_file: str):
        """Export session results to CSV for human review"""
        pass

    def export_json(self, output_file: str):
        """Export session results to JSON for AI processing"""
        pass
```

#### **Module 4: Data Normalizer** (`normalizer.py`)
**Purpose**: Standardize data format across platforms

```python
def normalize_listing(platform: str, raw_data: Dict) -> Dict:
    """
    Convert platform-specific listing data to standardized format.

    Standardized Schema:
    {
        "id": "unique-id",
        "platform": "facebook|offerup|craigslist",
        "title": str,
        "price": float,
        "currency": "USD",
        "location": str,
        "distance_miles": float,
        "url": str,
        "image_url": str,
        "posted_date": "YYYY-MM-DD",
        "tier1_data": {...},
        "tier2_data": {...} or null
    }
    """
    pass
```

#### **Module 5: CLI Interface** (`cli.py`)
**Purpose**: Command-line interface for AI to invoke

```python
import click

@click.group()
def cli():
    """Marketplace CLI - AI-Collaborative Data Capture Tool"""
    pass

@cli.command()
@click.option('--query', required=True, help='Search query')
@click.option('--location', required=True, help='Location (e.g., Philadelphia, PA)')
@click.option('--max-price', type=float, help='Maximum price')
@click.option('--platforms', multiple=True, help='Platforms to search')
@click.option('--all-platforms', is_flag=True, help='Search all platforms')
@click.option('--output', required=True, help='Output file path')
@click.option('--format', type=click.Choice(['json', 'csv']), default='json')
def search(query, location, max_price, platforms, all_platforms, output, format):
    """Execute tier 1 search across platforms"""
    pass

@cli.command()
@click.option('--input', required=True, help='Input results file')
@click.option('--indices', multiple=True, type=int, help='Listing indices to scrape')
@click.option('--output', required=True, help='Output file path')
def details(input, indices, output):
    """Execute tier 2 detailed scrape for specific listings"""
    pass

@cli.group()
def session():
    """Session management commands"""
    pass

@session.command('start')
@click.argument('session_id')
def session_start(session_id):
    """Start a new capture session"""
    pass

@session.command('add')
@click.argument('platform')
@click.argument('query')
@click.option('--max-price', type=float)
def session_add(platform, query, max_price):
    """Add search to current session"""
    pass

@session.command('run')
def session_run():
    """Execute all searches in current session"""
    pass

@session.command('export')
@click.option('--format', type=click.Choice(['json', 'csv']), required=True)
@click.option('--output', required=True)
def session_export(format, output):
    """Export session results"""
    pass
```

---

### Phase 3: AI Collaboration Features ðŸ¤–

#### **Feature 1: Natural Language Interface**
**AI Role**: Controller that translates natural language â†’ CLI parameters â†’ executes commands â†’ reads output files

**AI Usage Pattern**:
```
User: "Find me nintendo switches under $200 in Philadelphia"

AI Role as Controller:
1. Parse user input â†’ extract query, location, price
2. Normalize location â†’ "Philadelphia, PA" â†’ philly/Newark/jerseyshore codes
3. Construct CLI command with parameters:
   ```bash
   marketplace-cli search \
     --query "nintendo switch" \
     --location "Philadelphia, PA" \
     --max-price 200 \
     --all-platforms \
     --output results_20251105_143000.json \
     --format json
   ```
4. Execute command via Bash tool
5. **CLI tool does the work**: Scrapes platforms, normalizes data, creates JSON file
6. AI reads the generated results_20251105_143000.json file
7. AI summarizes findings to user

User: "Show me the top 5 cheapest ones"

AI Role as Controller:
1. Read results_20251105_143000.json (already generated by CLI)
2. Sort by price ascending
3. Display top 5 with title, price, platform, URL

User: "Get more details on #2 and #4"

AI Role as Controller:
1. Identify listings #2 and #4 from results file
2. Construct tier 2 command with specific indices:
   ```bash
   marketplace-cli details \
     --input results_20251105_143000.json \
     --indices 2 4 \
     --output details_20251105_143100.json
   ```
3. Execute command via Bash tool
4. **CLI tool does the work**: Clicks through URLs, scrapes details, creates JSON file
5. AI reads the generated details_20251105_143100.json file
6. AI presents condition, description, images to user
```

**Division of Labor**:
- **AI Responsibilities**: Natural language parsing, parameter extraction, command construction, file reading, result presentation
- **CLI Responsibilities**: Web scraping, data extraction, data normalization, CSV/JSON file generation, session persistence

#### **Feature 2: Session-Based Workflow**
**AI Role**: Controller that orchestrates multi-search sessions via CLI commands

**Use Case**: Large data capture across multiple searches

```
User: "I need to do market research on gaming consoles in the Philadelphia area"

AI Role as Controller:
1. Start session (CLI creates session directory structure):
   ```bash
   marketplace-cli session start "gaming_consoles_philly_nov2025"
   ```

2. Queue multiple searches (CLI stores search parameters):
   ```bash
   marketplace-cli session add facebook "nintendo switch" --max-price 300
   marketplace-cli session add facebook "playstation 5" --max-price 500
   marketplace-cli session add facebook "xbox series x" --max-price 500
   marketplace-cli session add offerup "nintendo switch" --max-price 300
   marketplace-cli session add offerup "playstation 5" --max-price 500
   marketplace-cli session add craigslist "nintendo switch" --max-price 300
   ```

3. Execute all searches (CLI scrapes all platforms, normalizes data):
   ```bash
   marketplace-cli session run
   ```

4. Export aggregated results (CLI merges data, generates CSV file):
   ```bash
   marketplace-cli session export --format csv --output gaming_console_market_research.csv
   ```

5. AI reads gaming_console_market_research.csv and reports summary to user

Division of Labor:
- AI: Interprets "market research" intent, constructs 6 search commands, triggers execution, reads output CSV
- CLI: Performs 6 web scrapes, normalizes all data, aggregates across platforms, generates CSV file
```

#### **Feature 3: LLM-Based Listing Analysis**
**AI Role**: Controller that triggers scraping, then analyzes results generated by CLI

**Use Case**: AI analyzes large result sets to find best matches

```
User: "Find listings that seem like good deals - looking for Switch with games included, good condition"

AI Role as Controller:
1. Execute tier 1 search (CLI scrapes, generates results.json):
   ```bash
   marketplace-cli search --query "nintendo switch" --location "Philadelphia, PA" \
     --max-price 300 --all-platforms --output results.json
   ```

2. Read results.json (generated by CLI)
3. Identify promising listings (price < $200)
4. Execute tier 2 scrape for promising listings (CLI clicks through, generates details.json):
   ```bash
   marketplace-cli details --input results.json --indices 2 5 8 12 --output details.json
   ```

5. Read details.json (generated by CLI)
6. Analyze each listing's description + condition
7. Present matches to user with reasoning

Division of Labor:
- AI: Natural language understanding ("good deals", "games included"), filtering logic, analysis of descriptions
- CLI: Web scraping (tier 1 & 2), data extraction, JSON file generation
```

#### **Feature 4: Price Monitoring**
**Use Case**: Track same search over time, alert on price drops

```python
# Future feature - not in Phase 2
class PriceMonitor:
    def watch_search(self, query: SearchQuery, check_interval_hours: int):
        """
        Re-run same search periodically, compare prices.
        Alert user if new listings appear or prices drop.
        """
        pass
```

---

### Phase 4: Testing and Validation âœ…

**Unit Tests**:
- [ ] Test natural language parser with diverse inputs
- [ ] Test each platform adapter independently
- [ ] Test data normalization across platforms
- [ ] Test session manager persistence
- [ ] Test CLI command parsing

**Integration Tests**:
- [ ] Test full tier 1 workflow (all platforms)
- [ ] Test full tier 2 workflow (specific listings)
- [ ] Test session-based workflow
- [ ] Test CSV export format
- [ ] Test JSON export format

**User Acceptance Tests**:
- [ ] User can provide natural language query, get results
- [ ] Results are accurate and complete
- [ ] CSV output is readable for human review
- [ ] JSON output is parseable by AI models
- [ ] Session management tracks large captures correctly

---

## Information Gathered So Far

### Test Results Summary
- **Location**: Pine Hill, NJ 08021
- **Test Query**: "nintendo switch"
- **Max Price**: $300

### Working Tools
1. **pyOfferUp**
   - Results: 43 listings
   - Price range: $30-$100
   - Location: Newark, NJ
   - Dependencies: requests only
   - Status: âœ… Production ready

2. **CraigslistScraper**
   - Results: 1 listing
   - Price: $250
   - Location: Jersey Shore, NJ (Brick, NJ)
   - Dependencies: requests, beautifulsoup4
   - Status: âš ï¸ Works but limited results

### Blocked/Broken Tools
1. **facebook-marketplace-scraper**
   - Error: `TargetClosedError: Target page, context or browser has been closed`
   - Issue: Facebook detects and blocks headless browsers
   - Technology: Playwright + BeautifulSoup + FastAPI + Streamlit
   - Status: âŒ Needs login/cookie solution

2. **Deals-Scraper**
   - eBay module: 0 results
   - Facebook module: Same login issue
   - Kijiji module: Canada-only
   - Status: âŒ Not viable (too many broken parts)

3. **PyCraigslist**
   - Issue: Requires FlareSolverr Docker container
   - Additional issue: cchardet doesn't compile on Python 3.12
   - Workaround: chardet can replace cchardet
   - Status: â¸ï¸ Needs Docker investigation

### Location Configuration for Pine Hill, NJ 08021
- **OfferUp**: Newark, NJ (available cities: Newark, Elizabeth, Jersey City)
- **OfferUp Coordinates**: lat=40.7357, lon=-74.1724
- **Craigslist**: jerseyshore site code
- **Nearest Major City**: Philadelphia
- **Facebook Marketplace**: Would use "philly" city code

---

## Files and Directories

### Test Scripts
Located in: `/Users/nes/projects/marketplace-cli/test_scripts/`
- `test_pyofferup.py` - âœ… Working
- `test_craigslistscraper_final.py` - âœ… Working
- `test_facebook_scraper.py` - âŒ Blocked
- `test_deals_scraper.py` - âŒ No results
- `test_pycraigslist.py` - â¸ï¸ Needs Docker

### Tool Directories
- `/craigslist-tools/PyCraigslist/` - On hold
- `/craigslist-tools/CraigslistScraper/` - Working
- `/offerup-tools/pyOfferUp/` - Working
- `/facebook-marketplace-tools/facebook-marketplace-scraper/` - Needs fix
- `/multi-platform-tools/Deals-Scraper/` - Abandoned

### Documentation
- `TEST_RESULTS.md` - Comprehensive test results
- `marketplace-cli-tools.md` - Tool evaluation checklist
- `README.md` - Project overview
- `.env.example` - API key templates

---

## Next Session Action Plan

### Immediate Tasks (Priority Order)

1. **Facebook Marketplace Investigation** (HIGHEST PRIORITY)
   - [ ] Read facebook-marketplace-scraper source code thoroughly
   - [ ] Test Streamlit GUI with manual browser login
   - [ ] Research cookie/session persistence with Playwright
   - [ ] Test headful mode: `browser = p.chromium.launch(headless=False)`
   - [ ] Look for Facebook Marketplace API documentation
   - [ ] Search GitHub for alternative Facebook Marketplace tools
   - [ ] Check if facebook-marketplace-scraper has open issues/PRs with fixes

2. **Craigslist Investigation** (HIGH PRIORITY)
   - [ ] Test CraigslistScraper with more search terms
   - [ ] Try different categories and locations
   - [ ] Manually visit Craigslist to compare results
   - [ ] Test PyCraigslist WITHOUT Docker to confirm it fails
   - [ ] Research Cloudflare bypass alternatives

3. **Docker Research** (MEDIUM PRIORITY)
   - [ ] Install Docker Desktop on macOS
   - [ ] Set up FlareSolverr container following documentation
   - [ ] Test PyCraigslist with FlareSolverr working
   - [ ] Document if performance/results are significantly better

### Questions to Answer

#### Facebook Marketplace
1. Does facebook-marketplace-scraper support saving/loading browser sessions?
2. Can we use Playwright's storage_state() to persist login?
3. Is there a Facebook Marketplace API (official or unofficial)?
4. Are there better/newer Facebook Marketplace scraping tools?

#### Craigslist
1. Why did CraigslistScraper only return 1 result? (Low inventory or parsing issue?)
2. Does PyCraigslist work at all without FlareSolverr?
3. What specific Cloudflare protection does Craigslist use?
4. Are there Cloudflare bypass libraries that work without Docker?

#### Architecture
1. Can unified CLI gracefully handle Docker dependencies?
2. Should we have "basic mode" (no Docker) vs "advanced mode" (Docker)?
3. How do we detect if Docker is installed and running?

---

## User Preferences & Constraints

### What User Wants
âœ… Facebook Marketplace working (top priority)
âœ… Craigslist working reliably
âœ… OfferUp working (already done)
âœ… Willing to provide credentials/API keys
âœ… Willing to use Docker if necessary
âœ… Wants to iron out kinks before building unified CLI

### What User Doesn't Want
âŒ To build unified CLI with broken components
âŒ To use tools that don't work for Pine Hill, NJ area
âŒ To skip investigation of potential fixes

---

## Success Criteria

Before proceeding to unified CLI design:

### Facebook Marketplace
- [ ] Can successfully scrape Facebook Marketplace for Pine Hill, NJ area
- [ ] Method is reliable and repeatable
- [ ] Solution is documented (login process, cookie management, etc.)

### Craigslist
- [ ] CraigslistScraper returns reasonable number of results (more than 1)
  OR
- [ ] PyCraigslist works with Docker and returns better results
- [ ] Docker setup is documented if required
- [ ] Fallback strategy is clear (PyCraigslist â†’ CraigslistScraper)

### OfferUp
- [x] Already working (43 results) âœ…

Once all three "lanes" are resolved, proceed to unified CLI design.

---

## Resources & Links

### Installed Tools
- pyOfferUp 0.3 (PyPI)
- craigslistscraper 1.1.2 (PyPI)
- playwright 1.40.0 (with chromium browser installed)
- streamlit 1.30.0
- fastapi 0.108.0

### Documentation
- Playwright: https://playwright.dev/python/docs/intro
- FlareSolverr: https://github.com/FlareSolverr/FlareSolverr
- Facebook Marketplace (unofficial): https://m.facebook.com/marketplace/directory/US/

### Potential Alternatives to Research
- `facebook-sdk` - Official Facebook SDK
- `selenium-stealth` - Bypass bot detection
- `undetected-chromedriver` - Selenium alternative
- `cloudscraper` - Cloudflare bypass without Docker
- `curl_cffi` - CURL with fingerprinting

---

## Notes for Future Self

### Context
User has limited tokens and may disconnect. This document contains everything needed to continue the investigation.

### User's Communication Style
- Direct and practical
- Willing to invest time in setup if it works
- Wants thorough investigation before moving forward
- Values working solutions over quick hacks

### Key Insight
User emphasized: *"I do not want to create the unified CLI until we can iron out some of these kinks"* - This means:
1. Don't rush to architecture design
2. Focus on making each tool work properly first
3. Investigate fixes and workarounds thoroughly
4. Document everything for reproducibility

### Remember
- Location: Pine Hill, NJ 08021
- Primary platforms: Facebook Marketplace, Craigslist, OfferUp
- User is OK with manual setup (Docker, login, API keys)
- Goal: Build reliable unified CLI once all components work

---

## Quick Start for Next Session

1. Read this entire document
2. Start with Facebook Marketplace investigation (highest priority)
3. Review facebook-marketplace-scraper code in detail
4. Test headful browser mode with manual login
5. Research session/cookie persistence
6. Document findings and report back to user

---

## ðŸ“‹ QUICK REFERENCE: AI-CLI Collaboration Model

### The Controller Pattern

**AI is the CONTROLLER** (interprets, decides, commands):
- âœ… Parses natural language from user
- âœ… Extracts parameters (query, location, price, platforms)
- âœ… Constructs CLI commands with flags/options
- âœ… Executes commands via Bash tool
- âœ… Reads generated output files (JSON/CSV)
- âœ… Analyzes, filters, sorts data from files
- âœ… Presents results to user

**CLI is the PROCESSOR** (scrapes, normalizes, generates):
- âœ… Performs web scraping (Playwright, Requests, BeautifulSoup)
- âœ… Extracts data from HTML
- âœ… Normalizes data across platforms
- âœ… Creates JSON files (structured data for AI)
- âœ… Creates CSV files (human-readable data)
- âœ… Manages sessions and persistence
- âœ… Handles authentication (Facebook login, etc.)

### Example Workflow

**User says**: "Find nintendo switches under $250 in Philadelphia"

**AI does**:
1. Extracts: query="nintendo switch", max_price=250, location="Philadelphia, PA"
2. Constructs: `marketplace-cli search --query "nintendo switch" --max-price 250 --location "Philadelphia, PA" --all-platforms --output results.json`
3. Executes command
4. Waits for CLI to finish
5. Reads results.json
6. Tells user: "Found 47 listings: 18 from Facebook ($80-$245), 15 from OfferUp ($95-$230), 14 from Craigslist ($100-$250)"

**CLI does** (when command runs):
1. Scrapes Facebook Marketplace (tier 1)
2. Scrapes OfferUp (tier 1)
3. Scrapes Craigslist (tier 1)
4. Normalizes all data to standard schema
5. Writes results.json with 47 listings
6. Exits with success code

**The AI never touches the scraping or file creation - it only pulls levers and reads outputs.**

---

## ðŸŽ¯ NEXT SESSION START HERE

### âœ… ALL THREE LANES RESOLVED - READY FOR CLI DEVELOPMENT

**Current Status**: All marketplace scrapers working perfectly
- âœ… Facebook: test_facebook_automated_search.py + test_facebook_listing_details.py
- âœ… OfferUp: pyOfferUp library
- âœ… Craigslist: test_craigslist_tier1.py + craigslist_scraper_patched/

### Phase 2: Build Unified CLI Architecture (START HERE)
1. **Set up project structure**
   - Create `adapters/` directory with base.py, facebook.py, offerup.py, craigslist.py
   - Create `cli.py` with Click commands
   - Create `session_manager.py` for data persistence
   - Create `normalizer.py` for standardized data format

2. **Implement platform adapters**
   - FacebookAdapter: Wrap test_facebook_automated_search.py logic
   - OfferUpAdapter: Wrap pyOfferUp library
   - CraigslistAdapter: Wrap CraigslistScraper

3. **Build CLI commands**
   - `marketplace-cli search` (tier 1 multi-platform)
   - `marketplace-cli details` (tier 2 specific listings)
   - `marketplace-cli session start/add/run/export` (session management)

4. **Implement standardized data format**
   - JSON schema for AI processing
   - CSV export for human review
   - URL preservation for follow-up

5. **Test AI collaboration workflow**
   - Natural language â†’ CLI parameters
   - Execute commands via Bash
   - Read output files
   - Present results to user

**End of Continuation Plan**
